## ğŸ›» IOT Data Streaming with Kafka, DuckDB, dlt & Snowflake
This project simulates connected vehicles streaming real-time IoT data, including GPS, weather, traffic incidents, and emergency alerts, all while following real-world routes. The data is sent to Kafka topics, enabling downstream analytical processing for optimization. This allows for efficient data handling and real-time insights.

### ğŸŒ Overview
This Python-based project models an IoT ecosystem where smart vehicles transmit:

âœ…GPS Location

âœ…Vehicle Information

âœ…Weather Conditions

âœ…Emergency Incident Data

âœ… Traffic Camera Feeds

All data is streamed via Apache Kafka, and vehicle routes are based on real geographical paths using the OpenRouteService API.

âš™ï¸ **Tech Stack**

| Layer       | Technology                   | Purpose                                                 |
|-------------|------------------------------|---------------------------------------------------------|
| ğŸ’¡ Simulation | Python, OpenRouteService API | Generate real-world vehicle data                        |
| ğŸ“¡ Streaming  | Apache Kafka                 | Publish sensor data in real time                        |
| ğŸ§  Storage    | DuckDB, Snowflake            | Stage and analyze structured data loccally and in prod) |
| ğŸ” Pipeline   | DLT (Data Loading Tool)      | Consuming and loading data to DUCKDB/Snowflake          |
| ğŸ§ª Testing    | Pytest                       | Validate generation + Kafka flow                        |


## ğŸ§­ How It Works
1. Simulated vehicles move between real cities (e.g. Potsdam â†’ Leipzig).

2. They emit various IoT-style data streams:

ğŸš˜ Vehicle telemetry

ğŸ“ GPS coordinates

ğŸš¦ Traffic camera events

ğŸŒ¤ï¸ Weather conditions

ğŸš¨ Emergency alerts


4. DLT listens to Kafka, and directly loads the data into:

âœ… DuckDB (local analytical DB for quick testing)

âœ… Snowflake (cloud warehouse for broader querying)

## ğŸš€ To replicate the project
1. Clone the repo
```
git clone https://github.com/your-name/iot-data-streaming.git
cd iot-data-streaming
```
2. Install dependencies
```
pip install -r requirements.txt
```
3. Configure environment
```
AWS_ACCESS_KEY_ID=''
AWS_SECRET_ACCESS_KEY=''
OPENROUTE_KEY=''

SOURCES__KAFKA__CREDENTIALS__BOOTSTRAP_SERVERS="localhost:9092"
SOURCES__KAFKA__CREDENTIALS__GROUP_ID=""
SOURCES__KAFKA__CREDENTIALS__SECURITY_PROTOCOL="PLAINTEXT"
SOURCES__KAFKA__CREDENTIALS__SASL_MECHANISMS="PLAIN"
SOURCES__KAFKA__CREDENTIALS__SASL_USERNAME=""
SOURCES__KAFKA__CREDENTIALS__SASL_PASSWORD=""

DESTINATION__SNOWFLAKE__CREDENTIALS__DATABASE=""
DESTINATION__SNOWFLAKE__CREDENTIALS__PASSWORD=""
DESTINATION__SNOWFLAKE__CREDENTIALS__USERNAME=""
DESTINATION__SNOWFLAKE__CREDENTIALS__HOST=""
DESTINATION__SNOWFLAKE__CREDENTIALS__WAREHOUSE=""
DESTINATION__SNOWFLAKE__CREDENTIALS__ROLE=""
```

4. Start Kafka and Zookeeper using Docker Compose:
```
docker-compose up -d
```
This will bring up the Kafka and Zookeeper containers.Kafka broker will be available on localhost:9093.

5. There are two steps in running:

âœ… Running the simulation for kafka to produce the data
```python -m src.simulation.main```

âœ… Running dlt to consume from kafka and write to duckDb/Snowflake. 
```python -m src.pipelines.dlt_pipelines```

If you're running with DuckDB, an iot_service.duckdb file will be created in the root directory, allowing for querying using a tool like DBeaver.



_In my case I consumed in batch whereby dlt consumes the available data with an append strategy to the database. The idea is to have a case where dlt run on a schedule._

ğŸ“ Project Structure
```
.
â”œâ”€â”€ docker-compose.yaml          # Optional: Start Kafka/Zoo locally
â”œâ”€â”€ iot_service.duckdb           # Local DuckDB file (DLT writes here- may not exist before run)
â”œâ”€â”€ replicate.md                 # Setup instructions for running locally
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ README.md                    # Project overview and usage
â”‚
â”œâ”€â”€ src/                         # Main project source code
â”‚   â”œâ”€â”€ config/                  # Env vars, API keys, ORS token, etc.
â”‚   â”‚   â””â”€â”€ config.py
â”‚   â”‚
â”‚   â”œâ”€â”€ kafka/                   # dlt-kafka helper functions
â”‚   â”‚   â””â”€â”€ helpers.py
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                  # Pydantic typed schemas (vehicle, gps, etc.)
â”‚   â”‚   â””â”€â”€ schema.py
â”‚   â”‚
â”‚   â”œâ”€â”€ pipelines/               # DLT pipelines
â”‚   â”‚   â””â”€â”€ dlt_pipeline.py      # Ingests from Kafka â†’ DuckDB â†’ Snowflake
â”‚   â”‚
â”‚   â”œâ”€â”€ simulation/              # Vehicle & sensor simulators
â”‚   â”‚   â”œâ”€â”€ constants.py         # Enum: fuel types, etc.
â”‚   â”‚   â”œâ”€â”€ generators.py        # Sensor data generators (GPS, traffic, weather, etc.)
â”‚   â”‚   â”œâ”€â”€ kafka_producer.py    # Kafka producer manager
â”‚   â”‚   â”œâ”€â”€ main.py              # Entry point: run vehicle simulation
â”‚   â”‚   â””â”€â”€ utils.py             # ORS helper client
â”‚
â”œâ”€â”€ tests/                       # Pytest-based unit tests
â”‚   â”œâ”€â”€ test_Kafka_producer.py
â”‚   â”œâ”€â”€ test_generators.py
â”‚   â”œâ”€â”€ conftest.py              # Test fixtures
â”‚
â”œâ”€â”€ legacy/                      # Deprecated Spark pipeline code (not used) 
â”‚   â”œâ”€â”€ spark_streaming.py       
â”‚   â”œâ”€â”€ schemas.py
â”‚   â””â”€â”€ main.py

```
## Key Learnings:
1. Simplified Data Pipeline with DLT and Kafka:

âœ… Switching from Spark to DLT simplified the pipeline. I no longer need to manage Spark workers, and schema evolution is handled automatically.

2. Efficient Local Analytics with DuckDB:

âœ… DuckDB enables fast, local analytics without the need for additional infrastructure. It processes large datasets efficiently, reducing both setup time and costs.

3. Seamless Integration with Snowflake:

âœ… DuckDB integrates well with Snowflake for scalable cloud storage, enabling smooth data transfers and analytics at scale.

4. Real-time Data Processing with Kafka:

âœ… Kafka ensures reliable and consistent real-time data streaming, while DLT manages schema changes and data integrity automatically.

5. Easy Data Management in DBeaver:

âœ… DBeaver simplifies database management by providing an intuitive interface to work with DuckDB and Snowflake. It helps in quickly querying and visualizing the data.

6. Non-Normalized Data Handling:

âœ… DLT allows me to store non-normalized IoT data when needed, making it easier to keep related data in a single table.

7. Scalability and Flexibility:

âœ… Using DuckDB for local data storage and Snowflake for scaling ensures flexibility. I can choose the right solution based on the dataset size.


## Summary:
âœ… The integration of **DLT** for real-time data processing, **DuckDB** for efficient local analytics, and **Snowflake** for scalable cloud storage has streamlined the data pipeline. 
âœ… The switch to Snowflake DB is seamless with DLT, allowing easy data loading and management without complex configurations. 
âœ… This setup reduces infrastructure complexity whi

## Areas for Improvement:
âœ… **Using an Orchestrator to Run DBT Models:** Incorporating an orchestrator like Airflow or Prefect could automate the scheduling and execution of DBT models, improving the overall pipeline management and reliability.

âœ… **Infrastructure as Code (IaC) for Snowflake Provisioning:** Adopting IaC tools like Terraform to provision Snowflake resources could ensure consistency, scalability, and easier management of cloud infrastructure.